{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkSQL_getting_started.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLezkqzcKGrE",
        "outputId": "17d35530-6de3-422b-ce83-548802be45c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 32 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 54.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=af013ca5a1ea04ffffee1d5fb9f563d28bca6dc1f0a2bad6734add417279c211\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[참고 사이트](https://spark.apache.org/docs/latest/sql-getting-started.html)\n"
      ],
      "metadata": {
        "id": "T-qsvs2JosZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Starting Point : SparkSession\n",
        "- spark 이용을 위한 entry point 역할"
      ],
      "metadata": {
        "id": "8T8p0NN5KLv8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ORyBwadMI3Uj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .appName(\"Python Spark SQL basic example\")\\ \n",
        "        .config(\"spark.some.config.option\", \"some-value\")\\ \n",
        "        .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json('./people.json') # json 파일 불러오기\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSNhaNphKCaz",
        "outputId": "57cbed61-7da4-49e9-e43b-818d1393cc0c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s75uki_WN8O9",
        "outputId": "f2995b1c-42ce-4328-bd92-1f00a515534b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. `select` : 칼럼 선택"
      ],
      "metadata": {
        "id": "Ba26HWZdSyC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 칼럼 선택\n",
        "df.select('name').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebhqtQ4YSTxL",
        "outputId": "db637d87-4f44-40fa-83a7-76efaab7e30b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 여러 셀 선택\n",
        "df.select(df['age']+1,df['name']).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um4cCA9HScqF",
        "outputId": "e565dd28-a486-44ca-f812-9b77600eb8d7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|(age + 1)|   name|\n",
            "+---------+-------+\n",
            "|     null|Michael|\n",
            "|       31|   Andy|\n",
            "|       20| Justin|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. `filter`"
      ],
      "metadata": {
        "id": "FM85szMAUfee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df['age']>21).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbDdausCSgzu",
        "outputId": "6d24f6a5-ac5c-4456-c752-bb7e2c2bd712"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "|age|name|\n",
            "+---+----+\n",
            "| 30|Andy|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. `groupBy`\n",
        "\n"
      ],
      "metadata": {
        "id": "pYVXObNcVKwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('age').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo19inMUVHbm",
        "outputId": "e9154fb4-e83d-4863-f94c-f28c5bd36358"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    1|\n",
            "|null|    1|\n",
            "|  30|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이외에도 다양한 명령어들이 있습니다!\n",
        "\n",
        "[더 많은 명령어 보기(head, isnull 등등 dataframe과 매우 유사한 함수 체계)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis)"
      ],
      "metadata": {
        "id": "XpYnFDSkWfP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL Query 날리기\n",
        "- Dataframe 등록\n",
        "- spark sql"
      ],
      "metadata": {
        "id": "TwYgTE0dXw4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL 쿼리사용을 위한 등록\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "\n",
        "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH0VOqhNWeyb",
        "outputId": "bbb3c363-eea0-4a0a-bdfd-ca48b5af9795"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Temporary View\n",
        "\n",
        "- spark SQL에서 temporary view는 Session안에만 적용되고 session 종료되면 사라짐.\n",
        "- "
      ],
      "metadata": {
        "id": "Mu5_1JuGYX93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createGlobalTempView(\"people\")\n"
      ],
      "metadata": {
        "id": "MU7yjeNwWMOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## txt(또는 csv) 파일로부터 schema 뽑아내기\n",
        "Reflection 사용하여 schema 추론하기"
      ],
      "metadata": {
        "id": "uL_WKsBpdMV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "lines = sc.textFile('./people.txt')# text file (comma로 구분되어 있음)\n",
        "\n",
        "parts = lines.map(lambda l : l.split(\",\"))# comma 로 된 데이터 split\n",
        "people = parts.map(lambda p : Row(name=p[0], age = int(p[1]))) # Row/age tuple 할당\n",
        "\n",
        "schemaPeople = spark.createDataFrame(people) # 데이터프레임 생성\n",
        "\n",
        "schemaPeople.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV5NsSfNdMC4",
        "outputId": "09966ba1-c407-46e4-843a-08a27b718ffa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|Michael| 29|\n",
            "|   Andy| 30|\n",
            "| Justin| 19|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teenagers = spark.sql(\"SELECT name FROM people WHERE age>=13 and age<=19\") # query로 할당\n",
        "\n",
        "teenNames = teenagers.rdd.map(lambda p : \"Name : \"+p.name).collect()# 모든 데이터 불러오기\n",
        "for name in teenNames:\n",
        "  print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOJ37_hwfRP4",
        "outputId": "d0b9e2fe-cb5b-4992-b396-702331d1d085"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name : Justin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Schema 특정하기\n",
        "1. 원래 RDD로부터 tuple이나 list의 RDD를 생성\n",
        "2. `StructureType`로 표현되는 schema 생성. 이것은 step1에서 생성된 RDD tuple이나 list를 매칭\n",
        "3. `createDataFrame`으로 RDD에 schema 적용. 이것은 `SparkSession`에 의해 제공된 것. "
      ],
      "metadata": {
        "id": "PuGSXvtfiUBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType, StructType, StructField # 자료형 정의\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "lines = sc.textFile('./people.txt')\n",
        "parts = lines.map(lambda l: l.split(\",\"))\n",
        "# 각 라인이 tuple로 변형됨\n",
        "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
        "\n",
        "# schema\n",
        "schemaString = 'name age'\n",
        "#field name : name, age\n",
        "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]#각 칼럼명을 Structfield로 감싸고\n",
        "schema = StructType(fields) # Structtype으로 최종 감싸기\n",
        "\n",
        "# schema RDD에 적용\n",
        "schemaPeople = spark.createDataFrame(people, schema)\n",
        "\n",
        "## temporary view 생성\n",
        "schemaPeople.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# query 날리기\n",
        "results = spark.sql(\"SELECT name FROM people\")\n",
        "\n",
        "results.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJgjnitZhinW",
        "outputId": "358d8fad-ca88-497d-c1fe-097c25171c59"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_ithY-ddku0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}